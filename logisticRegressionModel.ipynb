{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install rouge_score\n",
    "!pip install datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datasets import load_metric "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "De return_greater_than_min_num functie geeft een lijst met indexes van de zinnen uit de tekst die gekozen zijn voor in de samenvatting te gerbuiken.  \n",
    "Met volgende argumenten:\n",
    "- arr: Een lijst met per zin in de tekst de door het model gegenereerde relevantie waarde van die zin\n",
    "- thresh: De waarde waarboven elke zin ten minste moet zitten om in de samenvatting te mogen\n",
    "- min_num: Minimum nummer van indexes dat de return lijst moet bevatten \n",
    "- fix_num_flag: Boolean die aangeeft of er een vast aantal zinnen wordt gekozen voor de samenvatting of niet\n",
    "- fix_num: Wanneer de fix_num_flag True is dan zal de return lijst dit aantal van indexes bevatten(zolang ze boven de treshhold liggen). Zoniet dan wordt elke zin boven de treshhold gebruikt.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def return_greater_than_min_num(arr, thresh=0.5, min_num=1, fix_num_flag=False, fix_num=3):\n",
    "    if fix_num_flag == True:\n",
    "        idx = np.argsort(arr)[-fix_num:]\n",
    "        \n",
    "    else:\n",
    "        idx_prelim = np.where(arr>= thresh)\n",
    "    \n",
    "        if idx_prelim[0].shape[0] <= min_num:\n",
    "            idx = np.argsort(arr)[-min_num:]\n",
    "        else:\n",
    "            idx = idx_prelim\n",
    "            idx = idx[0]\n",
    "\n",
    "    return sorted(idx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "De return_df_pred_summaries functie geeft de gegenereerde samenvattingen terug in tekst.  \n",
    "Met megegeven argumenten: de test data, output van het model en de argumenten nodig voor de return_greater_than_min_num functie. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def return_df_pred_summaries( Xy_doc_label, y_pred, df_text, thresh, min_num,return_all=False, fix_num_flag=False, fix_num=3): \n",
    "    df_label_pred = pd.DataFrame({'doc_label': Xy_doc_label.flatten(),'y_pred': y_pred.flatten()}) \n",
    "    \n",
    "    df_label_pred = df_label_pred.groupby('doc_label').agg(list) \n",
    "\n",
    "    df_label_pred = df_label_pred.applymap(lambda x: np.array(x))\n",
    "\n",
    "    f = lambda arr: return_greater_than_min_num(arr, thresh=thresh, min_num=min_num,fix_num_flag = fix_num_flag, fix_num=fix_num)\n",
    "\n",
    "    df_label_pred = df_label_pred.applymap(f) \n",
    "\n",
    "    #Return predicted summary\n",
    "    df_doc = df_text[df_label_pred.index]\n",
    "    \n",
    "    \n",
    "    pred_summaries = [np.array(df_doc.iloc[j])[df_label_pred.iloc[j][0]].tolist()for j in range(len(df_label_pred))]\n",
    "\n",
    "    pred_summaries = [summ_list if type(summ_list) == str else ' '.join(summ_list) for summ_list in pred_summaries]\n",
    "    \n",
    "    if return_all == True:\n",
    "        answer = df_label_pred.values, df_label_pred.index, pred_summaries\n",
    "    else:\n",
    "        answer = pred_summaries\n",
    "    \n",
    "    return answer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Script"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Het definiÃ«ren van input, output en model file, de input file lezen en in een dataframe plaatsen.  \n",
    "Het scoring metric binnen halen. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_file = 'data/train_test_embed_only_df_processed_label_formatted.pickle'\n",
    "metric = load_metric(\"rouge\")\n",
    "output_file = 'data/results/logisticRegression_embed_only.pickle'\n",
    "model_file = 'data/results/logisticRegression_model.pickle'\n",
    "\n",
    "data_dict = pd.read_pickle(input_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al de nodige test en train data uit het dataframe halen"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Specify model inputs: df, X, y, doc_labels\n",
    "df = data_dict['df_original']\n",
    "train_test_set = data_dict['train_test_sets']\n",
    "# Specify train-test_data for validation        \n",
    "Xy_doc_label_train = train_test_set[0][0]\n",
    "Xy_doc_label_test = train_test_set[0][1]\n",
    "X_train = train_test_set[0][2]\n",
    "X_test = train_test_set[0][3]\n",
    "y_train = train_test_set[0][4]\n",
    "y_test = train_test_set[0][5]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Het logisticRegression model defineren en trainen.  \n",
    "Het model opslagen."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Define Model\n",
    "model = LogisticRegression(random_state=42,max_iter=10000)\n",
    "# Fit model\n",
    "model.fit(X_train,y_train.ravel())\n",
    "# Save model \n",
    "with open(model_file, 'wb') as handle:                                     \n",
    "    pickle.dump(model, handle)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Samenvattingen voor de test data genereren om dan hiermee het model met te scoren."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Predict Model\n",
    "y_pred = model.predict_proba(X_test)\n",
    "\n",
    "# Convert to binary predictions\n",
    "y_pred_bin = (y_pred >=0.5)*1\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_bin[:,1], labels=[0,1])\n",
    "\n",
    "# Return predicted summaries\n",
    "idx, doc_index, pred_summaries = return_df_pred_summaries(Xy_doc_label_test, y_pred[:,1], df.text_clean, thresh=0.5, min_num=1, return_all = True, fix_num_flag=True, fix_num=3)          \n",
    "\n",
    "# Match with gold summaries\n",
    "df_gold = df.sum_clean[doc_index]\n",
    "gold_summaries = [' '.join(df_gold .iloc[j]) for j in range(len(pred_summaries))]\n",
    "summaries_comp = tuple(zip(pred_summaries, gold_summaries))\n",
    "\n",
    "# Scores = calc_rouge_scores(pred_summaries, gold_summaries, keys=['rouge1', 'rougeL'], use_stemmer=True)\n",
    "metric.add_batch(predictions=pred_summaries,references=gold_summaries)\n",
    "\n",
    "scores = metric.compute()\n",
    "print(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Als laatste het resultaat van de scoring opslagen in output bestand.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "results_dict ={'conf_matrix': cm, 'summaries_comp': summaries_comp,\n",
    "               'sent_index_number': idx, 'Rouge': scores}\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(results_dict, handle)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('venv': venv)"
  },
  "interpreter": {
   "hash": "2cf193eb5eedc9af7d6b955fc1b1d6d96caf8b0b470f17c27fd55e50d2a0d0bf"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}